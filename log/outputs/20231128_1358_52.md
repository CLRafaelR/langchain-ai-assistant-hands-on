```
import torch
import datetime
from util import flush_memory
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

def generate(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    prompt: str,
    task: str,
    max_length: int = 4096,
    temperature: float = 0.6,
    top_p: float = 0.9,
) -> tuple[str, float]:
    """
    指定されたモデルとトークナイザーを使用して、与えられたプロンプトからテキストを生成し、その実行時間を計測する関数

    引数:
    model (AutoModelForCausalLM): 生成に使用する事前学習済みモデル
    tokenizer (AutoTokenizer): テキストをトークンに変換するためのトークナイザー
    prompt (str): モデルに入力するテキストプロンプト
    task (str): 実行するタスクの種類（'自由記述' または その他）
    max_length (int): モデルが扱う総入出力トークン数（初期値4096）
    temperature (float): 温度パラメタ（初期値0.6）
    top_p (float): top_pパラメタ（初期値0.9）

    戻り値:
    tuple: (生成されたテキスト, 実行にかかった時間（秒）)
    """

    # プロンプトをトークンIDにエンコード
    token_ids = tokenizer.encode(
        prompt,
        add_special_tokens=False,
        return_tensors="pt",
    )

    # プロンプトのトークン数を計算
    input_length = token_ids.size(1)

    # 出力可能な最大トークン数を計算
    outputable_length = max_length - input_length

    # 出力可能なトークン数が0以下の場合、エラーメッセージを返して処理を停止
    if outputable_length <= 0:
        flush_memory.flush()
        return ("Output length exceeds model's maximum capacity. Reduce input length.", 0.0)

    # タスクに応じて出力トークン数を設定
    if task == "自由記述":
        output_length = min(outputable_length, 1024)
        max_new_tokens = output_length
        min_new_tokens = int(output_length * 2 / 3)
    else:
        output_length = min(outputable_length, 50)
        max_new_tokens = output_length
        min_new_tokens = int(output_length * 1 / 5)

    # モデルによるテキスト生成の実行時間を計測
    start_time = datetime.datetime.now()
    try:
        with torch.no_grad():
            output_ids = model.generate(
                token_ids.to(model.device),
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                bos_token_id=1,
                do_sample=True,
                max_length=max_length,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                temperature=temperature,
                top_p=top_p,
            )
    except Exception as e:
        flush_memory.flush()
        return (f"An error occurred: {e}", 0.0)
    end_time = datetime.datetime.now()
    exec_time = (end_time - start_time).total_seconds()

    # 生成されたトークンIDをテキストにデコード
    output = tokenizer.decode(
        output_ids.tolist()[0][input_length:],
        skip_special_tokens=True,
    )

    # 使用したメモリを解放
    flush_memory.flush()
    return output, exec_time
```