```
    # Encode the prompt into token IDs
    token_ids = tokenizer.encode(
        prompt,
        add_special_tokens=False,
        return_tensors="pt",
    )

    # Calculate the number of tokens in the prompt
    input_length = token_ids.size(1)

    # Calculate the maximum number of tokens that can be output
    outputable_length = max_length - input_length

    # If the number of tokens that can be output is zero or less, return an error message and stop processing
    if outputable_length <= 0:
        flush_memory.flush()
        error_message = f"The input consists of {input_length} tokens, which surpasses the model's maximum capacity of {max_length} tokens. Please reduce the input by at least {input_length - max_length + 1} tokens."
        print(error_message)
        return (
            error_message,
            0.0,
        )
```