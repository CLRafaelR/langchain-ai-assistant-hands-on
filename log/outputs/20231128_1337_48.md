```
import torch
import datetime
from util import flush_memory

def generate(
    model,
    tokenizer,
    prompt,
    task,
):
    """
    指定されたモデルとトークナイザーを使用して、与えられたプロンプトからテキストを生成し、その実行時間を計測する関数

    引数:
    model (PreTrainedModel): 生成に使用する事前学習済みモデル
    tokenizer (PreTrainedTokenizer): テキストをトークンに変換するためのトークナイザー
    prompt (str): モデルに入力するテキストプロンプト
    task (str): 実行するタスクの種類（'自由記述' または その他）

    戻り値:
    tuple: (生成されたテキスト, 実行にかかった時間（秒）)
    """
    try:
        with torch.no_grad():
            # プロンプトをトークンIDにエンコード
            token_ids = tokenizer.encode(
                prompt,
                add_special_tokens=False,
                return_tensors="pt",
            )

            # モデルが処理する最大トークン数を設定
            max_length = 4096

            # プロンプトのトークン数を計算
            input_length = token_ids.size(1)

            # 出力可能な最大トークン数を計算
            outputable_length = max_length - input_length

            # タスクに応じて出力トークン数を設定
            if task == "自由記述":
                output_length = min(outputable_length, 1024)
                max_new_tokens = output_length
                min_new_tokens = int(output_length * 2 / 3)
            else:
                output_length = min(outputable_length, 50)
                max_new_tokens = output_length
                min_new_tokens = int(output_length * 1 / 5)

            # モデルによるテキスト生成の実行時間を計測
            start_time = datetime.datetime.now()
            output_ids = model.generate(
                token_ids.to(model.device),
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                bos_token_id=1,
                do_sample=True,
                max_length=max_length,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                temperature=0.6,
                top_p=0.9,
            )
            end_time = datetime.datetime.now()
            exec_time = (end_time - start_time).total_seconds()

            # 生成されたトークンIDをテキストにデコード
            output = tokenizer.decode(
                output_ids.tolist()[0][input_length:],
                skip_special_tokens=True,
            )
    except Exception as e:
        print(e)
        output = ""
        exec_time = 0
    finally:
        # 使用したメモリを解放
        flush_memory.flush()
        return output, exec_time
```