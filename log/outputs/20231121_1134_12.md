```python
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
import torch
import traceback

def load_model_tokenizer(
    model_name: str,
    cache_dir: str,
) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """モデルとトークナイザーを一括でロードする関数。

    引数:
        model_name (str): ロードするモデルの名称。
        cache_dir (str): モデルをダウンロードするディレクトリのパス。

    戻り値:
        tuple[AutoModelForCausalLM, AutoTokenizer]: ロードされたモデルとトークナイザーのタプル。
    """
    try:
        assert torch.cuda.is_available()

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map={"": 0},
            torch_dtype=torch.float16,
            use_cache=True,
            trust_remote_code=True,
            cache_dir=cache_dir,
        )

    except Exception as e:
        print(e)
        traceback.print_exc()
        print("GPUが利用できないため、モデルをCPUにロードします。")

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map={"": 0},
            device="auto",
            torch_dtype="auto",
            use_cache=True,
            trust_remote_code=True,
            cache_dir=cache_dir,
        )

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=cache_dir,
    )

    return model, tokenizer
```